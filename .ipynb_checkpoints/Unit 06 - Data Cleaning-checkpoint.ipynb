{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Data Cleaning\n",
    "\n",
    "Data cleaning sounds like an unglamorous task, but in reality, it is one of the most important things you will do in data science.  The fanciest machine learning algorithms will produce worthless models if your data isn't clean.  What do we mean by \"clean\"?  Broadly speaking, we mean that the data accurately represents what it claims to, and is formatted in a way that we can use our tools on it.  The topics we're going to look at in data cleaning include:\n",
    "* Formatting - is our data presented in a way that makes sense?  Are the variable types correct? (e.g.: is there a column of strings that are supposed to represent numbers?)\n",
    "* Outliers - for numerical data sets, are there any data points which differ greatly in value from most of the others that will affect the model?  (e.g.: does the price value jump from 91 to 920 to 91.5 over three seconds?).    \n",
    "* Missing data - are there any rows or columns that have blank, np.NaN, or otherwise missing data?  Should they be cut out?  If the data set is time-series, should we fill using nearby data?\n",
    "* Extra data - more common in time-series, has some process filled missing data already in a way that causes issues?\n",
    "* Repeated data / indices - are there any duplicates in our data where there shouldn't be?  Does a customer have two separate addresses listed when each should only have one?  For daily data, do any days appear more than once?\n",
    "* Re-indexing -  does the index accurately represent the data set?  Is it numerical when it should be datetime?\n",
    "* Data pre-processing - more accurately something to do after the data is \"clean\", do we need to add anything to the data that we need to use?  Should we create a flag to find out when two data fields match each other?  Do we need to add any computational indicators?\n",
    "\n",
    "As a visual example, if we know a specific set of financial instrument prices are supposed to look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"http://i.imgur.com/hfUxdZW.png\",width=600,height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we can't try to create a model if the data we receive looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"http://i.imgur.com/81oWrMV.png\",width=600,height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to figure out what problems are present, and the answers are not always obvious.  We'll return to using visualization as a data cleaning tool in a future unit, but for now we will use the Pandas tools we've already learned.  Let us load in the data set provided.\n",
    "\n",
    "## First Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (12.0, 10.0)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# This last line of code is because Pandas can be a bit... overzealous with warnings, and some \n",
    "# of the exercises presented here raised \"chained assignment\" warnings unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('c:\\\\users\\\\Patrick\\\\cleaning_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the first things we notice?  \n",
    "1. There's an extra column called \"index\".  We should probably make that the index\n",
    "2. The column called \"age\" has a collection of large numbers preceded by a 'u'.  This means their data type *isn't* a number.\n",
    "3. There's a column that has no title, which seems to contain ages.\n",
    "4. There's a column called \"Debt\" that has nothing in it.\n",
    "5. All of the column names are lower case except for the last.\n",
    "\n",
    "So, we can presume that the column names were shifted over one starting where age should be. Our first order of business will be to put the index right, reloading the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('c:\\\\users\\\\Patrick\\\\cleaning_data.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that looks a little better, but now we have two rows of column titles.  We don't need the index to have a title, so we can remove that with the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to change the column names to more accurately reflect the data present.  We saw we could call columns of a dataframe using the ```df.columns``` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can use this same command to rename the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = ['Name', 'Age', 'Debt', ' ']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the last column is empty, we'll redefine our dataframe to only be the columns that have data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[['Name','Age','Debt']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to look better, but the Debt column should be numbers, and currently the data type is not numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Debt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we fix this?  Well, Pandas interpreted those data points as strings, so we could just slice out everything from the second character onwards.  Luckily, Pandas provides us with a number of tools to do these things efficiently by vectorizing a lot of common string commands using ```.str```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Debt'].str[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So turning this column into numbers will be a two-step procedure:  \n",
    "1. We'll redefine the column using this string-parsing method.\n",
    "2. We'll use the Pandas function ```pd.to_numeric()``` change reasonable-looking data into numberic data-types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Debt'] = df['Debt'].str[1:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Debt'] = pd.to_numeric(df['Debt'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data set looks a lot cleaner, compared to where we started: the formats are all correct, the column names match the data, and it is properly indexed according to the file.  We could now confidently use this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "Statistically speaking, an outlier is a data point whose value differs significantly from other data points.  There are a number of reasons that outliers appear, including errors in data collection, and low-probability events in the sample. There is no precise definition of what an \"outlier\" is, as the context inside each data set is very important.  As a result, this means that observation is usually the first tool in addressing outliers.  We'll see in the upcoming units that there are also some statistical methods for dealing with outliers, depending on the distribution of the data.<br>\n",
    "We'll start with a constructed example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temps = pd.read_csv('c:\\\\users\\\\Patrick\\\\temperature_cleaning.csv',index_col=0,parse_dates=True)\n",
    "temps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a data set of average daily temperatures for May of 2017, in degrees Celsius.  Let's call the ```.describe()``` function to get an overview of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temps.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately see an issue.  The standard deviation of the data set is 42.2 (degrees Celsius), which is an immediate red flag.  We can also see that the max temperature in the set is apparently 240, which is likely an error.  Let's find all of the temperatures above 40 in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temps[temps['Avg Daily Temp']>40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that there is only one outlier.  We can slice out dates around it to see what the data's behaviour is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temps['2017-05-09':'2017-05-13']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to check for outliers is a visual inspection, by plotting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temps.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This not only shows us where the spikes are, but gives us an overview of where they are in relation to the rest of the data.\n",
    "\n",
    "The question of the best thing to do with an outlier, however, does not have a clear answer.  The temperature is listed as 240, and the median temperature of the data is 20; it's possible that this was a simple recording error, and that the real temperature is 24.  If you are confident in this, whether through consulting other sources, speaking with your team members, etc., you could change it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t2 = temps.copy() # We'll make a copy so we can address another method.\n",
    "temps.loc['2017-05-11','Avg Daily Temp'] = 24\n",
    "temps['2017-05-09':'2017-05-13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temps.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were not confident about changing it, owing to not having additional sources, etc., your best bet would likely be to drop the data point entirely.  Having spurious data in your set will only weaken the models you make.  The easiest way to accomplish this is by redefining the dataframe using querying; we'll use the copy we made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t2 = t2[t2['Avg Daily Temp']<40]\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t2.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes simple querying will not reveal where outliers are.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc = pd.DataFrame({'Close Price':[42, 39, 35, 31, 29, 24, 22, 28, 28, 24, 22, 43, 23, 26, 22, 26, 28, 26, 24, 22 ]},\n",
    "                  index=pd.date_range('2014-11-03',periods = 20,freq='D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very large spike on the 14th, comparable to the price at the beginning of the series.  If we tried just querying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc[pc['Close Price']>40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get both the spike, and the opening price (which seems reasonable, based on the graph).  One potential way to find spikes beyond just querying for values `>` or `<` than a threshold would be to see if the price jumps up and then jumps back down again, as some percentage of the previous and next prices.  Choosing an actual multiplier will depend on the data.  The average ratio of a price to the previous price is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(pc['Close Price']/pc['Close Price'].shift(1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we could look to see if there are any prices which are, say, 1.5 times more than the prices that come both before and after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc[(pc['Close Price'] > 1.5*pc['Close Price'].shift(1)) & (1.5*pc['Close Price'] > 1.5*pc['Close Price'].shift(-1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accurately singles out our spike, but a larger factor would not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc[(pc['Close Price'] > 2*pc['Close Price'].shift(1)) & (2*pc['Close Price'] > 1.5*pc['Close Price'].shift(-1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question of what to do with it remains:  it's not unreasonably large compared to the other prices (unlike the `10x` error), and it's certainly possible that a particularly volatile financial instrument may have very large price movements.  Secondary research would be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing / Extra Data\n",
    "\n",
    "One of the most common problems in data cleaning is missing data.  For static data sets, the usual way to deal with missing data is to drop the offending rows.  With time-series data, there are a few more options available:  we could sequentially fill the data forwards or backwards, so the index remains consistent.  <br>\n",
    "Occasionally you will find data sets (often time-series) that have extraneous data that has been filled in by some process, and will have to be cut out.  This occurs semi-regularly in financial price series data, where prices are filled through times where the contracts are not actually trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfm = pd.DataFrame({'Total Hours':[12,np.NaN,8,14,9.5,11],'Weekly Shifts':[2,3,1,2,1,3],\n",
    "                   'Overnight Shift':['N','N','Y','N','Y','Y',]},index=['Siva','J.R.','Nick','Keisha','Wayne','Judy'])\n",
    "dfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this employee overview, we can see that J.R. worked three shifts, but the total number of hours he worked is missing.  If we could find out from another source the total hours he worked, we could input the correct value with ```dfm.loc[]```.  If we were unable to find out the value, our best bet is to drop his information altogether.  This is accomplished with the ```dataframe.dropna()``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfm.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: again, this only returns a copy.  If we wanted to change the original, we would either have to redefine the dataframe, or include the argument ```inplace=True```.  There are a number of nice features in ```.dropna()```.  Let's create a new dataframe with more ```NaN```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan = pd.DataFrame({'a':[1,5,2,4],'b':[3,np.NaN,0,np.NaN],'c':[np.NaN,np.NaN,np.NaN,np.NaN],'d':[np.NaN,8,2,3]})\n",
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tell ```dropna``` to use ```axis=1``` (columns), we can drop columns where *all* the entries are ```NaN```, or *any* are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nan.dropna(axis=1,how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan.dropna(axis=1,how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case, only column ```'c'``` had every entry as ```NaN```, so it and only it was dropped. In the second case, every column *except* column ```'a'``` contained ```NaN```, so ```'a'``` was the only column to survive the \"any\" filter.<br>\n",
    "We can also provide a *threshold* for the minimum number of non-```NaN``` values present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan.dropna(axis=1,thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only column to not have 2 non-```NaN``` is column ```'c'```, so it was dropped.<br>\n",
    "If we set ```axis=0``` (or left it out, as it defaults to 0), we'd get a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan.dropna(axis=0,thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing was dropped, because every row has at least two non-```NaN```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at filling forward/backward.  Suppose we had a sequence of minute-level prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = pd.date_range('2017-05-01 09:30:00','2017-05-01 09:45:00',freq='min')\n",
    "pc = pd.DataFrame({'Price':[54,55,52,np.NaN,57,55,54,59,51,np.NaN,np.NaN,55,52,56,60,61]}, index=times)\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll make a few copies to show different methods\n",
    "pc2 = pc.copy()\n",
    "pc3 = pc.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could simply drop the missing values as we did before, but since this is a sequence of events, we could choose to fill the values with values nearby.  One reason for doing this is to keep the shape of your data the same, i.e.: keep the indices consistent, so there are no missing times.  An important consideration to make when choosing to fill is \"is it reasonable to think the values that are missing are close to the values around it?\"  (e.g.: minute to minute, the prices may be close to each other in value, but end of day prices may vary dramatically depending on the day; it may not be reasonable to fill forward at that level, and simply remove missing data).<br>\n",
    "To fill ```NaN``` values, we use Pandas ```dataframe.fillna()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method we used here was to fill forward, propagating values ahead in time across missing data.  We could also fill backwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc2.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another piece of functionality is to simply replace all of the missing data with a specific value.  This is similar to the querying we saw with ```.where()```.  If we filled with a blank string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc2.fillna(value='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue we can have with time-series is having values on dates or times that don't make sense for the data.  For example, if we had end-of-day profit totals for a store that was only open on weekdays, and it looked like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcw = pd.DataFrame({'Totals':[1607, 1438, 2476, 2489, 1587, 1587, 1587, 1509, 1919, 1682, 1984,2270, 2270, 2275]},\n",
    "                   index=pd.date_range('2017-02-13',periods=14,freq='D'))\n",
    "pcw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would know that there is a problem, because every day in the range is included, meaning Saturdays and Sundays too.  Luckily, Pandas parses dates very well, and internally indexes weekdays starting at 0 for Monday, and ending at 6 for Sunday.  So to cut out weekends, we can query the data with the condition that ```.index.weekday``` is less than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcw[pcw.index.weekday<5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we cut out just Mondays?  Or Tuesdays and Fridays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also cut out specific times of the day across all of the days we have.  Consider the following set of hourly open prices for a futures contract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ph = pd.DataFrame({'open':np.random.randint(2230,2295,size=49)},\n",
    "                   index = pd.date_range('2014-05-11 00:00:00','2014-05-13 00:00:00', freq='H'))\n",
    "ph.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we only cared about the prices during *Regular Trading Hours* (RTH) for this contract, which we'll say are between 10:00 and 15:00.  Well, we can slice out just those times over all of the days we have by using the ```dataframe.between_time()``` function.  It requires two arguments, the start and end times, and takes two optional arguments of whether to include the start/end times in the slice.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ph.between_time('10:00', '15:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is extremely useful, because the functionality in this single line of code (much like the timezone shifting) is something you would *never* want to program from scratch.<br>\n",
    "May the 11th 2014 was a Sunday.  How would we adjust our dataframe to be weekday RTH only?  (Recall, most of these operations make copies.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicated Data / Indices\n",
    "\n",
    "Rarely, we'll have rows or indices which are duplicated.  This can cause either errors in computations, or halt scripts that you've built because the size of the dataframe isn't the size the program expects it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup = pd.DataFrame({'Total Hours':[12,np.NaN,8,14,9.5,11,12],'Weekly Shifts':[2,3,1,2,1,3,2],\n",
    "        'Overnight Shift':['N','N','Y','N','Y','Y','N']},index=['Siva','J.R.','Nick','Keisha','Wayne','Judy','Siva'])\n",
    "dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Siva's row appears twice.  We can confirm this by using the ```.duplicated()``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix this error by using the ```.drop_duplicates()``` function.  We need to decide whether to keep the first or the last (or keep none of them, ```keep=False```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looked over the entire dataframe for duplicates, but we could be more specific about where to look.  We could eliminate all of the rows that have duplicated values in a specific column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup2 = pd.DataFrame({'a':[0,2,3,0],'b':['q','q','r','s']})\n",
    "dup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup2.drop_duplicates(subset='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we dropped all of the rows for which there were duplicated values in the ```'a'``` column (it defaults to keeping the first, if you do not specify).  Similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup2.drop_duplicates(subset='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have duplicated indices?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup2.index = [0,1,2,2]\n",
    "dup2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out where the index is duplicated by calling the ```.get_duplicates()``` on the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup2.index.get_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to deal with this, but the simplest is to do a query using the ```.duplicated``` function.  Here the ```~``` means \"not\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup2[~dup2.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query returns a Boolean array, as we saw above, giving True where the duplicated values appear (not including the first time they appear, because we wrote ```keep='first'```.  By putting the ```~```, we switch all of the truth values, and take everything except the duplicated indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup2.index.duplicated(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we do this is because we cannot call the drop_duplicates on the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup2.drop_duplicates(subset=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-indexing\n",
    "\n",
    "Occasionally our dataframe indices will become incorrect, either through the original state of the data (as we saw above), or or through concatenation.  Sometimes we will want to use an existing data column as the index.  We can accomplish this with the ```.set_index()```, and ```.reset_index()``` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfc1 = pd.DataFrame({'a':[0,2],'b':['q','q']})\n",
    "dfc2 = pd.DataFrame({'a':[3,0],'b':['r','s']})\n",
    "dfc = pd.concat([dfc1,dfc2])\n",
    "dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've concatenated the two dataframes, but the index is just the concatenation of the indices.  We can re-index the dataframe with an integer index using the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the ```drop=True``` means Pandas will not insert the old index as a new column.  This is handy to have in case your old index has useful information in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfc.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a column of information, and wanted to use it as an index, we could use ```.set_index()```.  First we'll create a new column, and then use it as an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfc['c'] = ['A','B','C','D']\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfc.set_index('c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This retains the extra row of column names, but we can get rid of that in the same way we did before.  Since all of these examples have been making *copies*, let's affect the original by redefining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfc = dfc.set_index('c')\n",
    "dfc.index.names=[None]\n",
    "dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last example, if we had a column of date-looking objects, we could convert them to date-time, and then use them as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfc['Dates'] = ['20160504','20160505','20160506','20160507']\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfc['Dates'] = pd.to_datetime(dfc['Dates'])\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfc = dfc.set_index('Dates')\n",
    "dfc.index.names = [None]\n",
    "dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the following data set, and set the dates as index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dft = pd.DataFrame({'Customers':[55,50,61,750,56,55],'Profit':[5455,5740,4430,6104,5650,np.NaN],\n",
    "                    'Employees':['7','8','7','10','11.5','7'],\n",
    "                    'Dates':['20160313','20160314','20160315','20160316','20160317','20160314']})\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data pre-processing\n",
    "\n",
    "Once our initial data set is \"clean\", we may need to add in additional information that we require for our model.  For example, we may have a list of current populations for different cities, and we may want to add in an additional column which tells us which country each city is located in.  We may have open-high-low-close-volume price information at the minute level for a futures contract we care about, but we may need to add in additional mathematical/statistical computations (\"indicators\") to be used for a trading strategy.\n",
    "\n",
    "The best tool to use for this is the ```np.where()``` function.  It's very similar to the Pandas dataframe function of the same name, and we will use it in the following way:<br>\n",
    "```df['newcolumn'] = np.where(condition, true value, false value)```.<br>\n",
    "Let's load in the open-high-low-close data we used in a previous unit, ```timeseries_1.csv```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ps = pd.read_csv('c:\\\\users\\\\Patrick\\\\timeseries_1.csv',index_col=0,parse_dates=True)\n",
    "ps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add an indicator column using the following formula:  if the absolute value of the open minus the close price is greater than 0.3, we'll call it \"Trend\", and if not we'll call it \"Range\".  The column will be called \"Indicator\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ps['Indicator'] = np.where(abs(ps['open']-ps['close'])>0.3,'Trend','Range')\n",
    "ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following temperatures for a given day for different cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp = pd.DataFrame({'City':['Vancouver','Toronto','Harrogate','Chennai','Quito'],\n",
    "                  'Temp High':[21,32,12,37,17],'Avg Temp':[22,25,20,28,14]})\n",
    "tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-index with the City names, and then introduce two new columns:  flags (True/False) if the recorded temp was more than 3 degrees away from the average temp, and more than 7 degrees away from the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6\n",
    "\n",
    "1. Consider the following dataframe of prices:<br>\n",
    "```pd.DataFrame({'price':[34.6, 35.66, 33.98, 38.67, 32.99, 32.04, 37.64, 38.22, 37.13, 38.57, 32.4, 34.98, 36.74, 32.9,32.52, 38.83, 33.9, 32.62, 38.93, 32.14, 33.09, 34.25, 34.39, 33.28, 38.13, 36.25, 38.91, 38.9, 36.85, 32.17, 32.07, 34.49, 35.7, 32.54, 37.91, 37.35, 32.05, 38.03, 0.32, 33.87, 33.16, 34.74, 32.47, 33.31, 34.54, 36.6, 36.09, 35.49, 37.51, 37.33, 37.54, 33.32, 35.09, 33.08, 38.3, 34.32, 37.01, 33.63, 36.35, 33.77, 33.74, 36.62, 36.74, 37.76, 35.58, 38.76, 36.57, 37.05, 35.33, 36.41, 35.54, 37.48, 36.22, 36.19, 36.43, 34.31, 34.85, 38.76, 38.52, 38.02, 36.67, 32.51, 321.6, 37.82,34.76, 33.55, 32.85, 32.99, 35.06]}, index = pd.date_range('2014-03-03 06:00','2014-03-06 22:00',freq='H'))```<br>\n",
    "    You are informed that the lowest price over this period is 32.04, and the highest was 38.93, and the data is in UTC.  Clean the data, change the time-zone to 'Asia/Singapore', and slice out the times that are between 09:00 and 14:30 (local time).\n",
    "2. Consider the following dataframe: <br>\n",
    "```pd.DataFrame({'day':['2015-05-05']*32 + ['2015-05-06']*29,'time':[' 08:00:00', ' 08:30:00', ' 09:00:00', ' 09:30:00', ' 10:00:00',' 10:30:00', ' 11:00:00', ' 11:30:00', ' 12:00:00', ' 12:30:00',' 13:00:00', ' 13:30:00', ' 14:00:00', ' 14:30:00', ' 15:00:00',' 15:30:00', ' 16:00:00', ' 16:30:00', ' 17:00:00', ' 17:30:00',' 18:00:00', ' 18:30:00', ' 19:00:00', ' 19:30:00', ' 20:00:00',' 20:30:00', ' 21:00:00', ' 21:30:00', ' 22:00:00', ' 22:30:00',' 23:00:00', ' 23:30:00', ' 00:00:00', ' 00:30:00', ' 01:00:00',' 01:30:00', ' 02:00:00', ' 02:30:00', ' 03:00:00', ' 03:30:00', '04:00:00', ' 04:30:00', ' 05:00:00', ' 05:30:00', ' 06:00:00',' 06:30:00', ' 07:00:00', ' 07:30:00', ' 08:00:00', ' 08:30:00',' 09:00:00', ' 09:30:00', ' 10:00:00', ' 10:30:00', ' 11:00:00',' 11:30:00', ' 12:00:00', ' 12:30:00', ' 13:00:00', ' 13:30:00',' 14:00:00'], 'Units Manufactured':np.random.randint(10,25,size=61)})```<br>\n",
    "The 'day' and 'time' columns are formatted as strings.  Create a new column which concatenates these columns into a single datetime-looking column, convert it to a column of datetime objects, and set it as the index.\n",
    "3. The *True Range* of a price series is defined as follows:<br>\n",
    "$$ \\text{TR} = \\max\\left[\\,\\left|(\\text{high}-\\text{low})\\right|,\\,\\left|(\\text{high}-\\text{close}_\\text{prev})\\right|,\\,\\left|(\\text{low}-\\text{close}_\\text{prev})\\right|\\,\\right] $$\n",
    "where $\\text{close}_\\text{prev}$ is the close of the previous time period.  Load in ```timeseries_1.csv``` again, and create a new column which is the True Range of the prices provided.\n",
    "5. (Bonus) You can apply a dictionary's rules to a column with ```dataframe.replace()``` by feeding it a new dictionary with the column name (as string) for the key, and the dictionary whose rules you want to apply as value.  Load in the included CSV ```unit_6_bonus.csv```, and combine the year, month, and day columns into a single datetime column, and apply it as the index.  Note:  the months are listed as three-letter names, not numbers.  Convert this column to the numbers corresponding with the months first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
